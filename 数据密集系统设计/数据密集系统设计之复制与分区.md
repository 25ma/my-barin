#分布式 #数据库 #数据结构 

# 数据复制

## 数据复制方式

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220610130604.png)

### 同步复制

#### 基础同步复制

主节点将数据发送给从节点，待从节点 ack 以后主节点再返回客户端 ack 信号。对于复制给从节点的数据，可以等到全部从节点 ack，亦可以部分从节点 ack 即可，但是至少需要一个从节点 ack 才能保证数据已被复制。

同步复制可以保证至少有一个副本备份了数据，缺点是从节点的延迟会阻塞主节点的处理直到从节点 ack。

#### 链式复制

### 异步复制

主节点将数据发送给从节点后不等待从节点 ack 即向客户端 ack，这样不能百分百保证数据被备份，但在某些要求高吞吐量的场景下此种方式被广泛使用，特别是那些从节点数量巨大或者分布千广域地理环境。

## 节点失效与切换

### 从节点失效

对于从节点失效，通常只需要等待节点重启后，从最后一次同步记录开始继续同步主节点数据即可。

### 主节点失效

主节点失效后，通常需要处理以下两个问题：主从切换，选择一个从节点将其提升为主节点；主节点切换，客户端需要往新主节点写入数据，其他从节点也要从新主节点同步数据。

主从切换可以手动切换或自动切换，自动切换步骤如下：

1. 确认主节点失效

	类似于系统崩溃，停电、网络问题等都会导致主节点失效，没有万无一失的方法可以检测问题如何产生，所以大多数系统都采用了基于超时的机制：节点间频繁地互相发生发送心跳存活消息，如果发现某一个节点在一段比较长时间内 (例如 30s) 没有响应，即认为该节点发生失效。

	对于超时时间的设定，也不是非常容易确定的，主节点失效后，超时时间设置得越长也意味着总体恢复时间就越长。但如果超时设置太短，可能会导致很多不必要的切换。一种选择的方式是平时记录系统的平均响应时间，根据其设定超时时间。

2. 选举新的主节点

	可以通过选举的方式 (超过多数的节点达成共识) 来选举新的主节点，或者由之前选定的某控制节点来指定新的主节点。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的风险。让所有节点同意新的主节点是个典型的共识问题。

3. 重新配置系统使新主节点生效

	客户端现在需要将写请求发送给新的主节点。如果原主节点之后重新上线，可能仍然自认为是主节点，而没有意识到其他节点已经达成共识迫使其下台。这时系统要确保原主节点降级为从节点，并认可新的主节点。

### 节点切换风险

#### 数据丢失问题

对于采用异步复制的方式，若主节点尚未完成复制就宕机，则此时从节点不会收到这部分数据。进行主从切换之后，对于未复制的数据，有以下两种处理：

1. 原主节点恢复后，将未复制的数据发送给新主节点。

	新主节点收到的复制数据可能与当前已有数据产生冲突，如采用数据自增 ID 生成了数据，原主节点又发来同样 ID 的数据。

2. 原主节点直接丢弃未复制的数据。

	对于这种处理，数据会直接丢失。

无论采用哪种方式，都会存在一定的问题，这是采用异步复制方式无法避免的。

#### 脑裂问题

在某些故障情况下，可能会发生两个节点同时都自认为是主节点的情况，这种情况被称为脑裂。它非常危险： 两个主节点都可能接受写请求，并且没有很好解决冲突的办法 , 最后数据可能会丢失或者破坏。作为一种安全应急方案，有些系统会采取措施来强制关闭其中一个节点。然而，如果设计或者实现考虑不周，可能会出现两个节点都被关闭的情况。

## 数据复制的实现

### 基于语句复制

最简单的情况，主节点记录所执行的每个写请求（操作语旬）并将该操作语旬作为日志发送给从节点。对千关系数据库，这意味着每个 INSERT、UPDATE 或 DELETE 语旬都会
转发给从节点，并且每个从节点都会分析并执行这些 SQL 语旬，如同它们是来自客户端那样。

语句级复制看起来是最简单的一种实现，逻辑紧凑，然而其有许多限制：

- 任何调用非确定性函数的语旬，如 NOW () 获取当前时间，或 RAND () 获取一个随机数等，可能会在不同的副本上产生不同的值。

- 如果语句中使用了自增列，或者依赖于数据库的现有数据（例如，`UPDATE ... WHERE CONDITION`），则所有副本必须按照完全相同的顺序执行，否则可能会带来不同的结果。进而，如果有多个同时并发执行的事务时，会有很大的限制。

- 有副作用的语句（例如，触发器、存储过程、用户定义的函数等），可能会在每个副本上产生不同的副作用。

对于语句级复制的限制，可以采取一定措施避免，如主节点可以在记录操作语句时将非确定性函数替换为执行之后的确定的结果，这样所有节点直接使用相同的结果值。但是，这里面存在太多边界条件需要考虑，因此目前通常首选的是其他复制实现方案。

### 基于预写日志 (WAL) 实现

可以采用 WAL 日志进行复制日志实现，如 Mysql 的 redolog 日志就是一种预写日志。其主要缺点是日志描述的数据结果非常底层：一个 WAL 包含了哪些磁盘块的哪些字节发生改变，诸如此类的细节。这使得复制方案和存储引敛紧密耦合。**如果数据库的存储格式从一个版本改为另一个版本，那么系统通常无法支持主从节点上运行不同版本的软件**。

看起来这似乎只是个有关实现方面的小细节，但可能对运营产生巨大的影响：

- 如果复制协议允许从节点的软件版本比主节点更新，则可以实现数据库软件的不停机升级： 首先升级从节点，然后执行主节点切换，使升级后的从节点成为新的主节点。 

- 相反，复制协议如果要求版本必须严格一致（例如 WAL 传输），那么就势必以停机为代价。

### 基于行的逻辑日志实现

另一种方法是复制和存储引擎采用不同的日志格式，这样复制与存储逻辑剥离。这种复制日志称为逻辑日志，以区分物理存储引擎的数据表示。关系数据库的逻辑日志通常是指一系列记录来描述数据表行级别的写请求：

- 对于行插入，日志包含所有相关列的新值。

- 对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值。

- 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少包含所有已更新列的新值）。

如果一条事务涉及多行的修改，则会产生多个这样的日志记录，并在后面跟着一条记录，指出该事务已经提交。 MySQL 的二进制日志 binlog (当配置为基于行的复制时） 使用该方式。

由于逻辑日志与存储引敛逻辑解耦，因此可以更容易地保持向后兼容，从而使主从节点能够运行不同版本的软件甚至是不同的存储引擎。

对于外部应用程序来说，逻辑日志格式也更容易解析。如果要将数据库的内容发送到外部系统（如用于离线分析的数据仓库），或构建自定义索引和缓存等，基于逻辑日志的复制更有优势。

### 基于触发器实现

到目前为止所描述的复制方法都是由数据库系统来实现的，不涉及任何应用程序代码。通常这是大家所渴望的，不过，在某些情况下，我们可能需要更高的灵活性。例如，只想复制数据的一部分，或者想从一种数据库复制到另一种数据库，或者需要订制、管理冲突解决逻辑，则需要将复制控制交给应用程序层。

触发器和存储过程：触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改（写事务）时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑，例如将数据更改复制到另一个系统。 Oracle 的 Databus 和 Postgres 的 Bucardo 就是这种技术的典型代表。

基于触发器的复制通常比其他复制方式开销更高，也比数据库内置复制更容易出错，或者暴露一些限制。然而，其高度灵活性仍有用武之地。

## 复制滞后问题

主从复制要求所有写请求都经由主节点，而任何副本只能接受只读查询。对于读操作密集的负载（如 Web) , 这是一个不错的选择： 创建多个从副本，将读请求分发给这些从副本，从而减轻主节点负载并允许读取请求就近满足。

在这种扩展体系下，只需添加更多的从副本，就可以提高读请求的服务吞吐最。但是，这种方法实际上只能用于异步复制，如果试图同步复制所有的从副本，则单个节点故障或网络中断将使整个系统无法写入。而且节点越多，发生故障的概率越高，所以完全同步的配置现实中反而非常不可靠。

不幸的是，如果一个应用正好从一个异步的从节点读取数据，而该副本落后于主节点，则应用可能会读到过期的信息。这会导致数据库中出现明显的不一致：由于并非所有的写入都反映在从副本上，如果同时对主节点和从节点发起相同的查询，可能会得到不同的结果。这种不一致只是一个暂时的状态，如果停止写数据库，经过一段时间之后，从节点最终会赶上并与主节点保持一致。这种效应也被称为最终一致性。

### 写后读 (read-after-write) 一致性

对于异步复制存在这样一个问题，如图所示，用户在写入不久即查看数据，则新数据可能尚未到达从节点。对用户来讲，看起来似乎是刚刚提交的数据丢失了：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611202301.png)

基千主从复制的系统实现写后读一致性有许多方案，如：

- 如果用户访问可能会被修改的内容，从主节点读取；否则，在从节点读取。

	这种判断方式就要求有一些方法在实际执行查询之前，就已经知道内容是否可能会被修改。

- 跟踪最近更新的时间，如果更新后一段时间内，则在主节点读取。

	如果应用的大部分内容都可能被所有用户修改，那么根据是否更新过从主从读取将不太有效，它会导致大部分内容都必须经由主节点，这就丧失了读操作的扩展性。

	此时需要其他方案来判断是否从主节点读取。例如，跟踪最近更新的时间，如果更新后一分钟之内，则总是在主节点读取；并监控从节点的复制滞后程度，避免从那些滞后时间超过一分钟的从节点读取。

- 记录更新时间戳保证提供的读服务至少包含了该时间戳的更新。

	客户端还可以记住最近更新时的时间戳，并附带在读请求中，据此信息，系统可以确保对该用户提供读服务时都应该至少包含了该时间戳的更新。如果不够新，要么交由另一个副本来处理，要么等待直到副本接收到了最近的更新。时间戳可以是逻辑时间戳（例如用来指示写入顺序的日志序列号）或实际系统时钟 (在这种情况下，时钟同步又称为一个关键点)。

	记住用户上次更新时间戳的方法实现起来可能会比较困难，因为在一台设备上运行的代码完全无法知道在其他设备上发生了什么。此时，元数据必须做到全局共享。

除了以上方式外，实现写后读一致还需要考虑其它的问题。若实现写后读一致性需要将请求都路由到主节点读取的话，这就要求在不同环境不同网络下数据都要路由到同一节点去读取。

### 单调读

单调读是一个比强一致性弱，但比最终一致性强的保证。当读取数据时，单调读保证，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611203711.png)

实现单调读的一种方式是，确保每个用户总是从固定的同一副本执行读取（而不同的用户可以从不同的副本读取）。例如，基于用户 ID 的啥希的方法而不是随机选择副本。但如果该副本发生失效，则用户的查询必须重新路由到另一个副本。

### 前缀一致读

前缀一致读保证对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序。

这是分区（分片）数据库中出现的一个特殊问题。如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常。然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611203941.png)

一个解决方案是确保任何具有因果顺序关系的写入都交给一个分区来完成，但该方案真实实现效率会大打折扣。现在有一些新的算法来显式地追踪事件因果关系，在本章稍后的 "Happened-before 关系与并发” 会继续该问题的探讨。

## 多主节点复制

主从复制存在一个明显的缺点： 系统只有一个主节点，而所有写入都必须经由主节点。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似： 处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点（也称为主主，或主动／主动）复制。此时，每个主节点还同时扮演其他主节点的从节点。

### 多主节点适用场景

#### 多数据中心

在多数据中心场景下，有了多主节点复制模型，则可以在每个数据中心都配置主节点。在每个数据中心内，采用常规的主从复制方案。而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611221745.png)

多主节点对比于单主节点对比如下：

1. 性能对比

	对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，并基本偏离了采用多数据中心的初衷（即就近访问）。而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。

2. 可用性对比

	1. 数据中心容错性

		对于主从复制，如果主节点所在的数据中心发生故陈，必须切换至另一个数据中心，将其中的一个从节点被提升为主节点。
		
		在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。
	
	2. 网络容错性

		数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。
		
		 多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功

3. 易用性对比

	对于多主节点，不同的数据中心可能会同时修改相同的数据，因而必须解决潜在的写冲突。

#### 离线客户端操作

另一种多主复制比较适合的场景是，应用在与网络断开后还需要继续工作。比如手机，笔记本电脑和其他设备上的日历应用程序。无论设备当前是否联网，都需要能够随时查看当前的会议安排（对应于读请求）或者添加新的会议（对应于写请求）。在离线状态下进行的任何更改，会在下次设备上线时，与服务器以及其他设备同步。

这种情况下，每个设备都有一个充当主节点的本地数据库（用来接受写请求），然后在所有设备之间采用异步方式同步这些多主节点上的副本，同步滞后可能是几小时或者数天，具体时间取决于设备何时可以再次联网。

从架构层面来看，上述设置基本上等同于数据中心之间的多主复制，只不过是个极端情况，即一个设备就是数据中心，而且它们之间的网络连接非常不可靠。多个设备同步日历的例子表明，多主节点可以得到想要的结果，但中间过程依然有很多的未知数。有一些工具可以使多主配置更为容易，如 CouchDB 就是为这种操作模式而设计的。

#### 在线协作

我们通常不会将协作编辑完全等价于数据库复制问题，但二者确实有很多相似之处。当一个用户编辑文档时，所做的更改会立即应用到本地副本 (Web 浏览器或客户端应用程序），然后异步复制到服务器以及编辑同一文档的其他用户。

如果要确保不会发生编辑冲突，则应用程序必须先将文档锁定，然后才能对其进行编辑。如果另一个用户想要编辑同一个文档，首先必须等到第一个用户提交修改并释放锁。这种协作模式相当千主从复制模型下在主节点上执行事务操作。为了加快协作编辑的效率，可编辑的粒度需要非常小。例如，单个按键甚至是全程无锁。然而另一方面，也会面临**所有多主复制都存在的挑战，即如何解决冲突**。

### 多主节点下写冲突处理

多主复制的最大问题是可能发生写冲突，这意味着必须有方案来解决冲突。

例如，两个用户同时编辑 Wiki 页面，如图 5-7 所示。用户 1 将页面的标题从 A 更改为 B, 与此同时用户 2 却将标题从 A 改为 C。每个用户的更改都顺利地提交到本地主节点。但是，当更改被异步复制到对方时，却发现存在冲突。注意，正常情况下的主从复制则不会出现这种情况。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611225330.png)

#### 同步/异步冲突检测

如果是主从复制数据库，第二个写请求要么会被阻塞直到第一个写完成，要么被中止 （用户必须重试）。然而在多主节点的复制模型下，这两个写请求都是成功的，并且只能在稍后的时间点上才能异步检测到冲突，那时再要求用户层来解决冲突为时已晚。

理论上，也可以做到同步冲突检测，即等待写请求完成对所有副本的同步，然后再通知用户写入成功。但是，这样做将会失去多主节点的主要优势： 允许每个主节点独立接受写请求。如果确实想要同步方式冲突检测，或许应该考虑采用单主节点的主从复制模型。

#### 冲突避免

处理冲突最理想的策略是避免发生冲突，即如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突。现实中，由于不少多主节点复制模型所实现的冲突解决方案存在瑕疵，因此，避免冲突反而成为大家普遍推荐的首选方案。

例如，一个应用系统中，用户需要更新自己的数据，那么我们确保特定用户的更新请求总是路由到特定的数据中心，并在该数据中心的主节点上进行读／写。不同的用户则可能对应不同的主数据中心（例如根据用户的地理位置来选择）。从用户的角度来看，这基本等价于主从复制模型。但是，有时可能需要改变事先指定的主节点，例如由于该数据中心发生故障，不得不将流量重新路由到其他数据中心，或者是因为用户已经漫游到另一个位置，因而更靠近新数据中心。此时，冲突避免方式不再有效，必须有措施来处理同时写入冲突的可能性。

#### 冲突解决

所有的复制模型至少应该确保数据在所有副本中最终状态一定是一致的。因此，数据库必须以一种收敛趋同的方式来解决冲突，这也意味着当所有更改最终被复制、同步之后，所有副本的最终值是相同的。

##### 固定冲突解决方案

- 给每个写入分配唯一的 ID, 例如，一个时间戳，一个足够长的随机数，一个 UUID 或者一个基于键－值的哈希，挑选最高 ID 的写入作为胜利者，并将其他写入丢弃。如果基于时间戳，这种技术被称为最后写入者获胜。虽然这种方法很流行，但是很容易造成数据丢失。

- 为每个副本分配一个唯一的 ID, 并制定规则，例如序号高的副本写入始终优先于序号低的副本。这种方法也可能会导致数据丢失。

- 以某种方式将这些值合并在一起。例如，按字母顺序排序，然后拼接在一起。

- 利用预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突（可能会提示用户）。

##### 自定义冲突解决方案

解决冲突最合适的方式可能还是依靠应用层，所以大多数多主节点复制模型都有工具来让用户编写应用代码来解决冲突。可以在写入时或在读取时执行这些代码逻辑：

- 在写入时执行

	只要数据库系统在复制变更日志时检测到冲突，就会调用应用层的冲突处理程序。例如， Bucardo 支持编写一段 Perl 代码。这个处理程序通常不能在线提示用户，而只能在后台运行，这样速度更快。

- 在读取时执行

	当检测到冲突时，所有冲突写入值都会暂时保存下来。下一次读取数据时，会将数据的多个版本读返回给应用层。应用层可能会提示用户或自动解决冲突，并将最后的结果返回到数据库。 CouchDB 采用了这样的处理方式。

注意，冲突解决通常用于单个行或文档，而不是整个事务。因此，如果有一个原子事务包含多个不同写请求，每个写请求仍然是分开考虑来解决冲突。

##### 自动冲突解决方案

有一些有意思的研究尝试自动解决并发修改所引起的冲突。下面这些方法值得一看：

- 元冲突的复制数据类型 (Conflict-free Replicated Datatypes, CRDT) 。 CRDT 是可以由多个用户同时编辑的数据结构，包括 map 、ordered list、计数器等，并且以内置的合理方式自动地解决冲突。一些 CRDT 已经在 Riak 2.0 中得以具体实现。

- 可合并的持久数据结构 (Mergeable persistent data)。它跟踪变更历史，类似于 Git 版本控制系统，并提出三向合并功能 (three-way merge function, CRDT 采用双向合并）。

- 操作转换 (Operational transformation)。它是 Etherpad 和 Google Docs 等协作编辑应用背后的冲突解决算法。专为可同时编辑的有序列表而设计，如文本文档的字符列表。

### 多主节点部署结构

复制的拓扑结构描述了写请求从一个节点的传播到其他节点的通信路径。如果有两个主节点，则只存在一个合理的拓扑结构： 主节点 1 必须把所有的写同步到主节点 2, 反之亦然。

但如果存在两个以上的主节点，则会有多个可能的同步拓扑结构：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611231109.png)

最常见的拓扑结构是全链接拓扑，如图 (c) , 每个主节点将其写入同步到其他所有主节点。而其他一些拓扑结构也有普遍使用，例如，默认情况下 MySQL 只支持环形拓扑结构, 其中的每个节点接收来自前序节点的写入，并将这些写入（加上自己的写入）转发给后序节点。另一种流行的拓扑是星形结构：一个指定的根节点将写入转发给所有其他节点。星形拓扑还可以推广到树状结构。

在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本，即中间节点需要转发从其他节点收到的数据变更。为防止无限循环，每个节点需要赋予一个唯一的标识符，在复制日志中的每个写请求都标记了已通过的节点标识符。如果某个节点收到了包含自身标识符的数据更改，表明该请求巳经被处理过，因此会忽略此变更请求，避免重复转发。

环形和星形拓扑的问题是，如果某一个节点发生了故障，在修复之前，会影响其他节点之间复制日志的转发。可以采用重新配置拓扑结构的方法暂时排除掉故障节点。在大多数部署中，这种重新配置必须手动完成。而对于链接更密集的拓扑（如全部到全部），消息可以沿着不同的路径传播，避免了单点故障，因而有更好的容错性。

全连接拓扑主要是存在某些网络链路比其他链路更快的情况（例如由于不同网络拥塞），从而导致复制日志之间的覆盖：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220611231538.png)

这里涉及到一个因果关系问题，类似于在[[数据密集系统设计/数据密集系统设计之复制与分区#前缀一致读|前缀一致读]]所看到的：更新操作一定是依赖于先前完成的插入，因此我们要确保所有节点上一定先接收插入日志，然后再处理更新。在每笔写日志里简单地添加时间戳还不够，主要因为无法确保时钟完全同步，因而无法在主节点 2 上正确地排序所收到日志。为了使得日志消息正确有序，可以使用一种称为版本向量的技术。

## 无主节点复制

