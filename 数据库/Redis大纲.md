#知识大纲 #Redis

# 1. Redis 大纲

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517160132.png)

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517160149.png)

# 2. 使用篇

## 2.1. Redis 的使用场景

- 缓存
- 计数相关：计数器/排行榜/浏览量/播放量等。
- 交并集操作：共同好友，朋友圈点赞。
- 简单消息队列：发布订阅。
- Session 服务器。
- 基于 RedisTimeSeries 模块操作时序数据库。

## 2.2. Redis 支持的数据结构

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518165826.png)

- String
- List
- Set
- Sorted Set
- Hash
- Bitmaps
- Hyperloglogs
- Geospatial

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518231920.png)

## 2.3. Redis 实现消息队列

消息队列设计需要满足以下几个需求：
- 消息顺序：支持按消息生产顺序消费。
- 消息持久化：支持消息的持久化，防止丢失。
- 消息重复消费：支持消息重复消费。

### 2.3.1. 基于 list 实现

1. Rpush 生产消息，lpop 消费消息，没有消息时 sleep 或者使用 blpop。
2. 为了支持消息重复消费，可以利用 List 类型提供的 BRPOPLPUSH 命令。这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存。这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了。同时需要注意消费成功后移除消息。
3. 消息支持多次消费，使用 pub / sub 模式可以达到 1: N 的消息队列
4. Pub / sub 模式的缺点：消费者下线的情况下，生产的消息会丢失
5. 延时队列的实现：基于 SortedSet，以时间戳为 score，消息内容作为 key 来生产消息。调用 zrangebyscore 来获取 N 秒前的数据。

### 2.3.2. 基于 Streams 实现

Redis 5.0 推出了 Stream 数据结构，它借鉴了 Kafka 的设计思想，弥补了 List 和 PubSub 的不足。Stream 类型数据可以持久化、支持 ack 机制、支持多个消费者、支持回溯消费，基本上实现了队列中间件大部分功能，比 List 和 PubSub 更可靠。

Streams 是 Redis 专门为消息队列设计的数据类型，它提供了丰富的消息队列操作命令。
- XADD：插入消息，保证有序，可以自动生成全局唯一 ID。
- XREAD：用于读取消息，可以按 ID 读取数据。同时支持阻塞式读取。
- XREADGROUP：按消费组形式读取消息。
- XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519121100.png)

## 2.4. Redis 实现分布式锁

- 使用 set/del 添加和释放锁。可带生效时间设定。
- 删除锁时判断线程 ID 是否是自己，防止误删除。
- 重入性使用 state 值来判断。
- 判断锁释放可以通过轮询或者 Redis 的发布订阅机制实现。

## 2.5. BigKey 有什么影响

- 网络阻塞，传输耗时长
- 超时阻塞，操作耗时长
- 内存占用高，空间分配不平衡

## 2.6. 如何查询固定前缀 key

- Keys 命令。会阻塞线程，查询事件复杂度是 o (n)，且查询结果是全量，不支持分页。KEYS 命令需要遍历存储的键值对，所以操作延时高。
- Scan 命令。不会阻塞线程，但有可能查出重复数据。不保证能得到查询期间被修改的元素。
	- 客户端通过执行 `SCAN $cursor COUNT $count` 可以得到一批 key 以及下一个游标\$cursor，然后把这个\$cursor 当作 SCAN 的参数，再次执行，以此往复，直到返回的$cursor 为 0 时，就把整个实例中的所有 key 遍历出来了。
	- 使用 SCAN 命令时，不会漏 key，但可能会得到重复的 key，这和 Redis 的 Rehash 机制有关：
		- 为什么不会漏 key？Redis 在 SCAN 遍历全局哈希表时，采用\高位进位\*的方式遍历哈希桶，当哈希表扩容后，通过这种算法遍历，旧哈希表中的数据映射到新哈希表，依旧会保留原来的先后顺序，这样就可以保证遍历时不会遗漏也不会重复。
		- 为什么 SCAN 会得到重复的 key？这个情况主要发生在哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个 key 返回多次。
	- SCAN 是遍历整个实例的所有 key，另外 Redis 针对 Hash/Set/Sorted Set 也提供了 HSCAN/SSCAN/ZSCAN 命令，用于遍历一个 key 中的所有元素，建议在获取一个 bigkey 的所有数据时使用，避免发生阻塞风险。
	-  使用 HSCAN/SSCAN/ZSCAN 命令，返回的元素数量与执行 SCAN 逻辑可能不同。执行 `SCAN \$cursor COUNT \$count` 时一次最多返回 count 个数的 key，数量不会超过 count。但 Hash/Set/Sorted Set 元素数量比较少时，底层会采用 intset/ziplist 方式存储，如果以这种方式存储，在执行 HSCAN/SSCAN/ZSCAN 命令时，会无视 count 参数，直接把所有元素一次性返回，也就是说，得到的元素数量是会大于 count 参数的。当底层转为哈希表或跳表存储时，才会真正使用发 count 参数，最多返回 count 个元素。

## 2.7. Redis 变慢的可能原因

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519164941.png)

redis-cli 命令提供了`–intrinsic-latency` 选项，可以用来监测和统计测试期间内的最大延迟。

### 2.7.1. 客户端相关的原因
- 客户端与 Redis 本身网络故障。
- API 使用不合理，如大 key 查询或 keys 扫描，集合统计聚合计算等。
- 设置的过期时间太集中，Redis 清理过期数据工作耗时长。

### 2.7.2. 服务端相关原因

#### 2.7.2.1. 与服务端机器性能相关

- 服务器 CPU 负载过高。
- 内存资源不足造成的内存淘汰与磁盘读取。
- 网络带宽过载。
- 频繁短连接，导致时间耗费在连接建立与释放过程上。应当使用长连接/连接池替代。
- 其它应用程序的资源争夺，更严重的产生内存与磁盘的 swap，导致性能下降。

#### 2.7.2.2. 与 Redis 应用相关

- 持久化过程占用资源过多
	- AOF 过程中若刷盘策略设置为 always，则每次更新操作都是 fsync 刷盘。并且此时若有 AOF 重写线程的话，这两个线程还会抢占 IO 资源，造成性能下降。如果我们需要高性能，同时也允许数据丢失，可以将配置项 `no-appendfsync-on-rewrite ` 设置为 yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。
	- 当策略是 EVERYSEC 时，主线程会创建子线程去完成 fsync 操作。但是当 io 压力大的时候，也就是 aof_buf 有积压时。主线程在 EVERYSEC 模式下会去判断是否有 aofwrite 在执行，并超过 2s。如果超过 2s 主线程不会 return，将继续等待。但是因为子线程在 aof_fd 上 fsync，所以 write aof_fd 的请求会被堵塞, 这里 write 全是主线程在操作，堵塞直到 fsync 完成。
	- 同时在 RDB 文件生成过程中，fork 时会拷贝页表。如果实例很大，那么拷贝过程耗时长。
	- 主从同步时，也会生成 RDB 文件。
- 正在进行过期数据删除操作，默认情况下，Redis 每 100 毫秒会删除一些过期 key
	- 采样 `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` 个数的 key，并将其中过期的 key 全部删除。ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 是 Redis 的一个参数，默认是 20，那么，一秒内基本有 200 个过期 key 会被删除。
	- 如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下。删除操作是阻塞的（Redis 4.0 后可以用异步线程机制来减少阻塞影响）。所以，一旦该条件触发，Redis 的线程就会一直执行删除，这样一来，就没办法正常服务其他的键值操作了，就会进一步引起其他键值操作的延迟增加，Redis 就会变慢。所以需要避免将过期时间设置在同一时刻。
- 内存大页：Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。在 Redis 持久化过程中，采用写时复制技术。内存大页会导致拷贝的页数据变大。
- 多核 CPU 运行时，若 Redis 与 CPU 核心绑定不合理，也会造成性能下降。
- Redis 数据频繁操作，导致内存碎片的产生。碎片整理 Redis 提供了许多可配置的选项，碎片整理是在主线程进行的，所以需要评估影响进行合适的配置。
	- 通过 `info memory` 查看碎片情况，`mem_fragmentation_ratio` 指标系统分配给 Redis 的内存和实际使用的内存的比值。正常情况下这个比值大于 1，当小于 1 的时候就意味着部分数据被 swap 到磁盘去了，这个时候如果读取这些数据就要从磁盘加载到内存了。
	- `active-defrag-cycle-min 25`： 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展。
	- `active-defrag-cycle-max 75`：表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
	- `active-defrag-ignore-bytes 100mb`：表示内存碎片的字节数达到 100MB 时，开始清理。
	- `active-defrag-threshold-lower 10`：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。

### 2.7.3. 慢日志分析

可以使用 Redis 日志（慢查询日志）和 latency monitor 来排查执行较慢的命令操作。在使用慢查询日志前，我们需要设置两个参数：
- slowlog-log-slower-than：慢查询日志对执行时间大于多少微秒的命令进行记录。
- slowlog-max-len：慢查询日志最多能记录多少条命令记录。

我们可以使用 `SLOWLOG GET` 命令，来查看慢查询日志中记录的命令操作。Redis 从 2.8.13 版本开始，还提供了 latency monitor 监控工具，这个工具可以用来监控 Redis 运行过程中的峰值延迟情况。这个工具是通过扫描数据库来查找 bigkey 的，所以，在执行的过程中，会对 Redis 实例的性能产生影响。

### 2.7.4. bigkey 排查

- Redis 可以在执行 redis-cli 命令时带上–bigkeys 选项，进而对整个数据库中的键值对大小情况进行统计分析。
- 使用 scan 命令扫描 key，然后手动统计。可以使用 `MEMORY USAGE` 命令查询某个 key 占用的内存空间。
- 利用第三方工具，如 `rdb-tools`。这是基于 rdb 文件统计的一个工具。

## 2.8. 缓存问题

### 2.8.1. 缓存雪崩

- 原因：由于大批量的缓存突然失效导致请求都打到了数据库上
- 措施
	- 缓存失效时间分开，设置随机失效时间
	- 控制数据库写入操作，只允许一个线程写入

### 2.8.2. 缓存穿透

- 原因：查询缓存中没有的数据，请求打到了数据库
- 措施
	- 使用布隆过滤器拦截
	- 缓存空的数据，并设置过期时间

### 2.8.3. 缓存预热

- 原理：自动将热点数据加载到缓存中

### 2.8.4. 缓存更新

#### 2.8.4.1. Cache-Aside 旁路缓存模式

做法：
- 读请求
	1. 先查询缓存
	2. 缓存中有直接返回
	3. 缓存中没有查询数据库更新缓存
- 写请求
	1. 先更新数据库
	2. 然后删除缓存

选择原因：
- 为何不先删除缓存后更新数据库？
	- 若先删除缓存，在缓存删除期间产生读请求，可能会将未更新的数据查询到缓存中导致缓存脏数据。
- 若选择先删除缓存后更新数据库，如何解决一致性问题？
	- 采用延时双删，更新数据库后延时一段时间再次删除缓存，总共删除两次。这种做法不但需要两次删除而且有延迟，所以不推荐使用。
- 为何是删除缓存而不是更新缓存？
	- 若产生两个并发的写请求，因为各种原因导致先来请求缓存更新操作晚于后来的请求，同样会导致缓存脏数据。
- Cache-Aside 存在数据不一致的可能吗？
	- 存在，若缓存失效期间同时产生写请求与读请求，且读请求的缓存更新操作晚于写请求的缓存删除操作，这个时候也会出现数据不一致问题。这种情况要求缓存失效且读写同时触发，条件比较复杂。
	- 另一种情况是读请求先读到了缓存，写请求更新了数据。这个时候缓存与数据库数据不一致，也是一种一致性问题。若是业务对此要求严格一致，可采取加锁方式解决。

Cache-Aside 的异常补偿机制：
- 当删除缓存时存在缓存删除失败的问题，需要作出补偿策略。
- 删除重试机制，同步删除重试影响性能，因此可以使用异步重试删除，如使用 MQ。但这样引入了 MQ 中间件，以及删除失败后但逻辑需要业务代码但 trigger 触发。
- 监听 binlog 日志，解析 binlog 日志来处理缓存删除失败的问题。注意这里优先选择监听从数据库的 binlog 日志，防止主节点事务还未完成就过早的删除了缓存。
- 采用云服务商提供的 DTS (数据传输) 服务，DTS 服务适配了常见的数据源与数据操作场景，解决了如 binlog 日志回收，主备切换场景下的高可用问题。

适用点：
- 缓存数据计算逻辑复杂
- 数据一致性要求高
- 不存在大 key 或热点数据

#### 2.8.4.2. Read-Through 读穿透模式

流程和 Cache-Aside 模式相似，不同点在于 Read-Through 多了访问控制层，读请求只和访问控制层交换，缓存能否命中与读请求无关。

#### 2.8.4.3. Write-Through 直写模式

同样提供了访问控制层来进行更高程度的封装。不同于 Cache-Aside 模式的是 Write-Through 不是删除缓存而是更新缓存。该模式适合写操作多且对一致性要求高的场景。

#### 2.8.4.4. Write-Behind 异步回写模式

- 写请求只更新缓存不更新数据库，数据库在合适的时机异步批量更新。
- 这种模式写延迟低，吞吐性好，但一致性弱，需要缓存做好高可用，适合于大量的写请求场景。如秒杀，MQ 的消息存储机制等。

#### 2.8.4.5. Write-Around

- 写请求不更新缓存，缓存设定失效时间，由失效时间自动更新。这种模式适用于一致性要求不高的业务场景。

### 2.8.5. 缓存降级

缓存失效或缓存服务器挂掉的情况下不去访问数据库直接返回内存数据或默认数据。以此减少降级对业务对影响操作。

# 3. 高性能

## 3.1. Redis 效率高的原因

- C 语言实现，效率高。
- 纯内存操作。
- 基于非阻塞式的 IO 复用模型。
	- 基于 select/epoll 的事件回调机制，使 Redis 可以在网络 IO 阻塞时处理其他事情。
	- 由操作系统内核与客户端建立连接，Redis 监听这些连接的事件进行处理。
	- 注意这里时网络 IO 操作非阻塞的，对于 Redis 的 server 层来说，如果阻塞的话则会影响后面的所有命令。比如 bigkey 操作，复杂命令的执行，缓存淘汰和刷盘机制等。
- 单线程避免上下文切换，各种锁操作。
	- Redis 的瓶颈往往在于内存和网络带宽，不在 CPU。
	- 单线程避免调度切换开销。
- 丰富的数据结构，对数据存储做了优化，如压缩表，跳表。

## 3.2. Redis 为什么是单线程的

- Redis中的多线程
	- 在 Redis4.0 中引入了多线程处理异步任务
	- 在 Redis6.0 中对网络模型实现了多线程 IO，用于处理网络数据对读写和协议解析。主线程将可读 Socket 分发给 IO 线程组进行并行请求解析，解析完毕的命令执行还是单线程的，执行结果交给 IO 线程组写回 Socket 是并行的。即将与 Socket 相关的读写操作变为并行执行的，以此减轻网络 IO 的负担。但是命令执行还是由主线程执行的，仍是单线程，且是线程安全的。

## 3.3. Redis 数据结构

### 3.3.1. Redis 的数据存储结构

#### 3.3.1.1. RedisObject

对于Redis的数据，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。对于存储在 Redis 中的对象，其结构如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518222536.png)

- 元数据信息占 8 字节
- 真实数据指针占 8 字节

为了节省内存空间，Redis 还对 Long 类型整数和 SDS 的内存布局做了专门的设计。
- 当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。
- 当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为 embstr 编码方式。
- 当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。这种布局方式被称为 raw 编码模式。
- 上述 44 字节的计算方式为：len、cap 以及新增的 flag 使用的都是 int8 类型，只占用 1 个字节，这样 64 (cpu cache line 为 64)-16（ReadObj 头部）-3（sds 头部）-1（buf 末尾'\\0'）= 44。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518223052.png)

#### 3.3.1.2. Hash dictEntry

Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518223353.png)


![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518165826.png)

### 3.3.2. 简单动态字符串

字符串类型在Redis中的实现为简单动态字符串(Simple Dynamic String，SDS)格式，其存储结构如下：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518222037.png)

- buf：字节数组，保存实际数据。数组最后额外的一个'\\0'，用于表示字节数组的结束。这会额外占用 1 个字节的开销。
- len：占 4 个字节，表示 buf 的已用长度。新版的SDS使用int8存储，只占1字节。
- alloc：也占个 4 字节，表示 buf 的实际分配长度，一般大于 len。新版的 SDS 使用 int8 存储，只占 1 字节。

### 3.3.3. 压缩列表

#### 3.3.3.1. 压缩列表的定义

- 压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数。压缩列表在表尾还有一个 zlend，表示列表结束。
- 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O (1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O (N) 了。
- 压缩列表中每个数据可以具有不同的长度，因此具有**压缩**的特性。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518172301.png)

#### 3.3.3.2. 压缩列表的存储

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518224610.png)

压缩列表之所以能节省内存，在于它是用一系列连续的 entry 保存数据。每个 entry 的元数据包括下面几部分：
- prev_len，表示前一个 entry 的长度。
	- prev_len 有两种取值情况：
		- 1 字节：取值 1 字节时，表示上一个 entry 的长度小于 254 字节。
			- 当上一个 entry 长度小于 254 字节时，prev_len 取值为 1 字节。
			- 对于 254，用于表示 5 字节长度开始的标识。
			- 对于 255，为压缩列表中 zlend 的取值默认。因此，就默认用 255 表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。
		- 5 字节：由两部分组成：
			- 第一部分为 1 字节，值为 254，表示这是一段 5 字节表示的数据。
			- 第二部分为 4 字节，实际的长度。
- encoding：表示编码方式，1 字节。
- data：保存实际数据。

这些 entry 连续在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。

压缩列表可以被用于 Hash 结构的实现，但当超过以下阈值时，Hash 结构将不再使用压缩列表而转为哈希表：
- `hash-max-ziplist-entries`：表示用压缩列表保存时哈希集合中的最大元素个数。
- `hash-max-ziplist-value`：表示用压缩列表保存时哈希集合中单个元素的最大长度。

我们往 Hash 集合中写入的元素个数超过了 `hash-max-ziplist-entries`，或者写入的单个元素大小超过了 `hash-max-ziplist-value`，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表。

#### 3.3.3.3. 压缩列表的使用事项

- 当使用压缩列表存储时，我们尽量存储 int 数据，压缩列表在设计时每个 entry 都进行了优化，针对要存储的数据，会尽量选择占用内存小的方式存储（整数比字符串在存储时占用内存更小），这也有利于我们节省 Redis 的内存。
- 压缩列表是每个元素紧凑排列，而且每个元素存储了上一个元素的长度，所以当修改其中一个元素超过一定大小时，会引发多个元素的级联调整（前面一个元素发生大的变动，后面的元素都要重新排列位置，重新分配内存），这也会引发性能问题，需要注意。
- 采用压缩列表方式存储时，虽然可以节省内存空间，但是在查询指定元素时，都要遍历整个 ziplist，找到指定的元素。所以使用压缩列表方式存储时，虽然可以利用 CPU 高速缓存，但也不适合存储过多的数据（`hash-max-ziplist-entries` 和 `zset-max-ziplist-entries` 不宜设置过大），否则查询性能就会下降比较厉害

### 3.3.4. 跳表

跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518172416.png)

### 3.3.5. BitMap

Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。

可以把 Bitmap 看作是一个 bit 数组。Bitmap 提供了 GETBIT/SETBIT 操作，使用一个偏移值 offset 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，Bitmap 的偏移量是从 0 开始算的，也就是说 offset 的最小值是 0。当使用 SETBIT 对一个 bit 位进行写操作时，这个 bit 位会被设置为 1。Bitmap 还提供了 BITCOUNT 操作，用来统计这个 bit 数组中所有“1”的个数。

### 3.3.6. HyperLogLog

HyperLogLog 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小。

在 Redis 中，每个 HyperLogLog 只需要花费 12 KB 内存，就可以计算接近 2\^64 个元素的基数。

HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。

### 3.3.7. Geospatial

GEO 类型的底层数据结构就是用 Sorted Set 来实现的，其中 set 的 key 为数据的 key，而 value 为经纬度的编码值。

为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 GeoHash 编码方法，这个方法的基本原理就是“二分区间，区间编码”。

在进行第一次二分区时，经度范围\[-180, 180]会被分成两个子区间：\[-180, 0) 和\[0, 180]（我称之为左、右分区）。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示。

这样一来，每做完一次二分区，我们就可以得到 1 位编码值。然后，我们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234740.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234746.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518234819.png)

当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，其中，偶数位从 0 开始，奇数位从 1 开始。

用了 GeoHash 编码后，原来无法用一个权重分数表示的一组经纬度（116.37，39.86）就可以用 1110011101 这一个值来表示，就可以保存为 Sorted Set 的权重分数了。

使用 GeoHash 编码后，我们相当于把整个地理空间划分成了一个个方格，每个方格对应了 GeoHash 中的一个分区。所以，我们使用 Sorted Set 范围查询得到的相近编码值，在实际的地理空间上，也是相邻的方格，这就可以实现 LBS 应用“搜索附近的人或物”的功能了。

有的编码值虽然在大小上接近，但实际对应的方格却距离比较远。例如，我们用 4 位来做 GeoHash 编码，把经度区间\[-180, 180]和纬度区间\[-90, 90]各分成了 4 个分区，一共 16 个分区，对应了 16 个方格。编码值为 0111 和 1000 的两个方格就离得比较远，如下图所示：所以，为了避免查询不准确问题，我们可以同时查询给定经纬度所在的方格周围的 4 个或 8 个方格。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518235015.png)

## 3.4. Pipline

- 将多次 IO 压缩成一次，但是要求管道中的指令没有因果关系。
- 使用 pipline 实现请求/响应的功能。客户端未读取服务端响应时服务端可处理新的请求。客户端发送多个命令时只需等待服务端最终结果。

## 3.5. 引起 Redis 阻塞的几种操作

- 集合全量查询和聚合操作。
- bigkey 删除，可以利用子线程进行，称之为 lazy-free。
- 清空数据库。
- AOF 日志同步写，可以利用子线程进行。
- 从库加载 RDB 文件。

lazy-free 是 4.0 新增的功能，但是默认是关闭的，需要手动开启。手动开启 lazy-free 时，有 4 个选项可以控制，分别对应不同场景下，要不要开启异步释放内存机制： 
- lazyfree-lazy-expire：key 在过期删除时尝试异步释放内存。
- lazyfree-lazy-eviction：内存达到 maxmemory 并设置了淘汰策略时尝试异步释放内存。
- lazyfree-lazy-server-del：执行 RENAME/MOVE 等命令或需要覆盖一个 key 时，删除旧 key 尝试异步释放内存。
- replica-lazy-flush：主从全量同步，从库清空数据库时异步释放内存

### 3.5.1. Lazy-free

即使开启了 lazy-free，如果直接使用 DEL 命令还是会同步删除 key，只有使用 UNLINK 命令才会可能异步删除 key。

上面提到开启 lazy-free 的场景，除了 `replica-lazy-flush` 之外，其他情况都只是可能去异步释放 key 的内存，并不是每次必定异步释放内存的。 

开启 lazy-free 后，Redis 在释放一个 key 的内存时，首先会评估代价，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行（不同线程传递数据也会有性能消耗）。什么情况才会真正异步释放内存？这和 key 的类型、编码方式、元素数量都有关系（详细可参考源码中的 `lazyfreeGetFreeEffort` 函数）：
- 当 Hash/Set 底层采用哈希表存储（非 ziplist/int 编码存储）时，并且元素数量超过 64 个。
- 当 ZSet 底层采用跳表存储（非 ziplist 编码存储）时，并且元素数量超过 64 个。
- 当 List 链表节点数量超过 64 个（注意，不是元素数量，而是链表节点的数量，List 的实现是在每个节点包含了若干个元素的数据，这些元素采用 ziplist 存储）。

只有以上这些情况，在删除 key 释放内存时，才会真正放到异步线程中执行，其他情况一律还是在主线程操作。也就是说 String（不管内存占用多大）、List（少量元素）、Set（int 编码存储）、Hash/ZSet（ziplist 编码存储）这些情况下的 key 在释放内存时，依旧在主线程中操作。 

可见，即使开启了lazy-free，String类型的bigkey，在删除时依旧有阻塞主线程的风险。所以，即便Redis提供了lazy-free，还是尽量不要在Redis中存储bigkey。 **Redis在设计评估释放内存的代价时，不是看key的内存占用有多少，而是关注释放内存时的工作量有多大**。从上面分析基本能看出，如果需要释放的内存是连续的，Redis作者认为释放内存的代价比较低，就放在主线程做。如果释放的内存不连续（大量指针类型的数据），这个代价就比较高，所以才会放在异步线程中去执行。

## 3.6. Redis 高效利用 CPU 的方式

### 3.6.1. CPU 架构之 NUMA 架构

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519125629.png)

多核 CPU 通过共享总线对内存进行访问，那么对总线对争用将会称为性能对阻碍。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519125740.png)

Non-Uniform Memory Access，NUMA 架构通过把 CPU 和临近的 RAM 当做一个 node，CPU 会优先访问距离近的 RAM。同时，CPU 直接有一个快速通道连接，所以每个 CPU 还是访问到所有的 RAM 位置（只是速度会有差异）。

采用 NUMA 架构虽然减少了总线的争用，但是若程序发生了 CPU 的切换，那么 CPU 中的缓存数据就要重新加载，这是 NUMA 架构存在的问题：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130036.png)

### 3.6.2. Redis 进行 CPU 层面的优化

#### 3.6.2.1. 绑定 CPU 与 Redis 程序

如果在 CPU 多核场景下，Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了。每调度一次，一些请求就会受到运行时信息、指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求。因此可以把 Redis 与运行的 CPU 进行绑定，避免 CPU 的切换带来的数据重载问题：

```bash

taskset -c 0 ./redis-server

```

 Redis 实例和网络中断程序的数据交互：网络中断处理程序从网卡硬件中读取数据，并把数据写入到操作系统内核维护的一块内存缓冲区。内核会通过 epoll 机制触发事件，通知 Redis 实例，Redis 实例再把数据从内核的内存缓冲区拷贝到自己的内存空间。那么，在 CPU 的 NUMA 架构下，当网络中断处理程序、Redis 实例分别和 CPU 核绑定后，就会有一个潜在的风险：如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访问内存，这个过程会花费较多时间。
 
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130422.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130432.png)

所以，为了避免 Redis 跨 CPU Socket 访问网络数据，我们最好把网络中断程序和 Redis 实例绑在同一个 CPU Socket 上，这样一来，Redis 实例就可以直接从本地内存读取网络数据了，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519130503.png)

#### 3.6.2.2. 绑核带来的问题

当我们把 Redis 实例绑到一个 CPU 逻辑核上时，就会导致子进程、后台线程和 Redis 主线程竞争 CPU 资源，一旦子进程或后台线程占用 CPU 时，主线程就会被阻塞，导致 Redis 请求延迟增加。

对于上述情况，我们可以采取以下策略优化：
- 一个 Redis 实例对应绑一个物理核：在给 Redis 实例绑核时，不要把一个实例和一个逻辑核绑定，而要和一个物理核绑定。也就是说，把一个物理核的 2 个逻辑核都用上。
- 优化 Redis 源码：通过修改 Redis 源码，把子进程和后台线程绑到不同的 CPU 核上。Redis 6.0 后，可以支持 CPU 核绑定的配置操作了。

虽然与 CPU 进行关系绑定可以在一定程度上可以带来收益，但是我们仍需要关注 CPU 层面的使用情况。若绑定的 CPU 处于忙碌状态，上面已经绑定了其他的程序，那么 CPU 缓存的公用就会带来淘汰问题。

# 4. 高可用

## 4.1. Redis 的主从同步

### 4.1.1. 主从同步的过程

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517231216.png)

主从同步用于备份主库的数据，从库通过与主库建立连接而不断获取到主库到数据。建立连接的步骤如下：
1. 从库使用 replicaof/slaveof (5.0 之前) 来开始主库建立连接。
2. 从库发送 psync 命令给主库建立连接，首次建立连接 runId 为 -1，offset 为 -1。runId 为机器 ID，offset 为复制进度，首次 -1 表示从头开始。
3. 主库通过 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。
4. 主库执行 bgsave 命令得到 RDB 文件给从库。从库接收到文件后清空当前数据，然后解析 RDB 恢复数据。使用RDB文件有以下几点原因：
	- RDB 文件是压缩过的二进制文件，文件内容小。对于网络传输快。
	- RDB 文件直接使用二进制协议解析还原数据，避免 AOF 文件的命令重复执行。
5. 对于生成 RDB 文件期间和从库解析 RDB 文件期间产生到数据操作命令，记录到了 replication buffer 中，待后续从库解析完后再根据这些命令补充数据。
6. 首次备份完毕后主从之间通过长连接维护通信，主库将每次写入命令都推送到从库。

### 4.1.2. 主从同步的重连

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517234150.png)

主从连接建立完毕以后就可以正常同步数据了，但是当主从之间因为某些原因断开重新建立连接后，双方数据怎么继续同步呢？

对于主库，在每次收到写入操作后，会将命令存入到 `repl_backlog_buffer` 环形缓冲区。主从之间的通信，会有一个 `replication buffer`。`repl_backlog_buffer` 用于记录主库的写入命令，对于主库来说只有一个。而 `replication buffer` 是对每一个从库都有的，这用于主从之间发送缓冲数据使用。这两者之间更为详细的对比如下：
- `repl_backlog_buffer` 是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。各个从节点的 offset 偏移量都是相对该缓冲区而言的。若一个从库都没有，那么这个缓冲区存在就没有意义了，此时会被释放。
- `replication buffer` 是为了主从之间数据通信**缓冲**所使用的，就像 Mysql 中的 net_buffer。这个 buffer 是针对每一个从库都会有一个的。不管是全量同步还是增量同步，都会使用到这个缓冲区。
- 如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个 `replication buffer` 就会持续增长，消耗大量的内存资源，甚至 OOM。所以 Redis 提供了 `client-output-buffer-limit` 参数限制这个 buffer 的大小，如果超过限制，主库会强制断开这个 client 的连接，也就是说从库处理慢导致主库内存 buffer 的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。

对于缓冲区 `repl_backlog_buffer`，主库使用 `master_repl_offset` 来记录写入的位置。从库使用 `slave_repl_offset` 记录自己从主库读取数据的位置。这样在主从之间数据通信的时候，只需要将两个 offset 之间的数据进行同步即可。

对于主从重新建立连接后，有两种情况：
1. 主库的 `master_repl_offset` 还未追赶上从库的 `slave_repl_offset`，这时只要发送 offset 之间的数据即可。
2. 而如果主库的 `master_repl_offset` 已经追赶上从库的 `slave_repl_offset`，那么将会进行数据的覆盖。数据覆盖后从库就不能通过 offset 进行数据同步了，此时只能进行全量数据同步。
3. 因此需要特别留意 `repl_backlog_size` 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。

对于 `repl_backlog_buffer` 的大小，可以通过 `repl_backlog_size` 参数进行控制。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 \* 操作大小 - 主从库间网络传输命令速度 \* 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。

## 4.2. 哨兵机制

基于主从模式，我们可以将数据分散到多个节点。主节点可以接收读写请求，从节点接收读请求。但是当主从之间出问题时该如何处理呢？从节点挂了影响尚小，可要是主节点挂了呢？我们根据什么判断主节点挂了，主节点挂了以后将哪个从库推选为主节点，如何将新的主节点通知给其他从库呢？

### 4.2.1. 哨兵机制的概念

哨兵是一个处于特殊模式的 Redis 进程，它负责以下三个任务：
1. 监控：监控主库运行状态，并判断主库是否客观下线
	1. 哨兵会周期性的给所有主从库发送 ping 命令以检测服务是否在线。如果检测失败，就会被标记下线。若主库被标记下线，启动自动选举主库流程。
	2. 对于从库，哨兵对从库的 ping 超时未回复就可以将其标记为下线。
	3. 对于主库，若某个哨兵 ping 超时则将其标记为主观下线。需要多个哨兵都认为下线了才可将其标记成客观下线，此时主库真的会下线，开始执行选主流程。
	4. 简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。

1. 选主：在主库客观下线后，选取新主库
	1. 筛选出不满足条件的节点，如除了检查从库的当前在线状态，还要判断它之前的网络连接状态。若一个节点过去频繁超时掉线，这个节点被认为不可靠。例如配置 `down-after-milliseconds * 10` 控制超时时间与次数。
	2. 哨兵将从剩余的从节点中挑选出一个新的主节点。选主时会综合判断从节点的状态，哨兵将会从多个维度对从节点进行打分操作，选择一个分高的从节点作为新的主节点。
	3. 第一轮会从优先级判断，若手动给从库设置了高优先级，则高优先级优先。
	4. 优先级相同对按照与旧主库的数据同步进度，同步进度快的优先。哨兵监视期间，从库会使用 info 命令将自身信息同步给哨兵，其中就包括 offset 信息。
	5. 进度相同的情况下按照从库ID号优先。

1. 通知：选出新主库后，通知从库和客户端
	1. 哨兵将新的主节点通知给其他从节点，让从节点与新的主节点开始数据同步。
	2. 哨兵会把新主库的地址写入自己实例的 pubsub（switch-master）中。客户端需要订阅这个 pubsub，当这个 pubsub 有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。
	3. 客户端需要访问主从库时，不能直接写死主从库的地址，需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name 命令）。这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

### 4.2.2. 哨兵集群

我们可以通过哨兵来进行主从数据库的监控选举与通知，但是如果哨兵节点挂机了要怎么办呢？因此我们需要部署多个哨兵构建哨兵集群来保证哨兵的高可用。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518115624.png)
![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518115632.png)

哨兵集群的原理：
1. 哨兵通过 Redis 的发布订阅机制来获取 Redis 主库信息以及和其他哨兵建立联系。
	1. 哨兵首先与主库建立连接，监控主库信息。
	2. 除了自身以外，哨兵还会通过发布订阅机制从主库获取到其他哨兵节点构建哨兵集群。
	3. 所有哨兵通过主库的 `__sentinel__:hello` 频道来互相发现与通信。
2. 哨兵连接到主库后，就可以获取到主库信息和其他哨兵信息了。对于从库，哨兵会通过 info 命令从主库获取到所有的从库，从而再次和从库建立连接进行监听。
3. 除了从库外，哨兵还需要和客户端建立连接，以通知客户端主库切换信息，以及客户端关心的其他信息。如通过 `+switch-master` 命令得知主库地址切换，`+sdown` 实例进入主观下线状态等。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518120328.png)

### 4.2.3. 哨兵选主

当主库被标记为客观下线后，就会开始进行主库的切换。那么由哪个哨兵来进行这个切换的过程呢？这就需要哨兵选举出一个 leader，由 leader 来进行切换操作：
1. 首先需要将主库标记为客观下线状态，这需要哨兵集群中大多数哨兵达成共识。
	1. 任何一个哨兵只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。一个哨兵获得了仲裁所需的赞成票数 `quorum` 后，就可以标记主库为“客观下线”。
	2. 这里就要求所有的哨兵将主库的 `down-after-milliseconds` 参数设置为一致，否则就会出现下线行为认为不一致的现象。
2. 主库被标记为下线状态后，哨兵集群需要选举出一个 leader 来进行切换操作。称为leader的如下：
	1. 拿到半数以上的赞成票。**这意味着若有超过半数以上哨兵节点宕机，则选举操作无法进行。**所以我们要求哨兵节点数量至少为 3，否则若一台哨兵节点宕机，则无法进行选举。当然哨兵节点数量也不要过多，否则会造成节点选举耗时时间长。
	2. 拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。
3. 选举出 leader 之后，由 leader 完成主库的切换操作。

哨兵 leader 的选举过程：
1. 当哨兵节点判断主库下线后，会向其他哨兵节点发送投票通知让其他节点投票给自己。同时该哨兵也会给自己投一票。
2. 若一个哨兵节点接收到投票通知后，自身还未投过票，则统一该通知，返回 Y。否则若已经给自己和其他节点投过票，返回 N。
3. 投票完成后若有节点获得超过半数以上赞成票且票数大于 quorum 值，则其称为 leader，否则将会再次选举。选举周期为哨兵故障转移超时时间。

## 4.3. Redis 的持久化方式

### 4.3.1. AOF (Append-only-file)

#### 4.3.1.1. AOF 原理

- 原理：将所有命令以 Redis 命令请求协议存储，保存为 AOF 文件。命令保存时机有以下配置：
	- Always：同步写回。每个写命令执行完，立马同步地将日志写回磁盘。崩溃时会丢失一次事件循环的命令。在同步时下一次事件循环才会把上一次的事件产生的缓冲 fsync 同步到磁盘中。
	- Everysec：每秒写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘。崩溃时会丢失一秒内的命令。
	- No：操作系统控制的写回。每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。崩溃时会丢失不可控数据的命令。

- 优点
	- 数据安全，一次操作就可以备份一次
	- 通过 append 模式，即使中途宕机也可以回复数据
	- 有 rewrite 模式，当 aof 文件过大时可以进行命令合并
- 缺点
	- AOF 文件偏大，恢复慢
	- 数据集大时，比 RDB 启动效率低

#### 4.3.1.2. AOF重写

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517181251.png)

当操作命令积累时，AOF 文件会变大。AOF 重写支持已当前数据库的所有数据，对其生成每一条 set 命令，从而产生一个全新的 AOF 文件。重写机制具有“多变一”功能，该 AOF 文件理论上会比原始文件小许多，这个操作称为 AOF 重写。

每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写。然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。

但是 AOF 重写存在阻塞主线程的风险：
1. fork 这个瞬间一定是会阻塞主线程的，fork 采用操作系统提供的写实复制 (Copy On Write) 机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题。但 fork 子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量 CPU 资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork 阻塞时间越久。
2. 拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。“写实复制”是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险。
	1. fork 出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行 AOF 重写，把内存中的所有数据写入到 AOF 文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的 key，那么这个时候父进程就会真正拷贝这个 key 对应的内存数据，申请新的内存空间。这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。
	2. 因为内存分配是以页为单位进行分配的，默认4k。如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在**Redis机器上需要关闭Huge Page机制**。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。

AOF 重写的配置：
- `auto-aof-rewrite-min-size 64mb` 配置 AOF 文件运行时最大容量。
- `auto-aof-rewrite-percentage 100` 配置 AOF 文件运行时本次 AOF 文件与上一次 AOF 文件体积增量比。
- 在 aof 文件体量超过 `auto-aof-rewrite-min-size`，且比上次重写后的体量增加了 `auto-aof-rewrite-min-size` 时自动触发重写。
- 可以手动发送 `bgrewriteaof` 指令触发一次重写。

Redis4.0 以后，Redis 的 AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头，将增量的以指令的方式 Append 到 AOF，这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。缺点是 AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。

### 4.3.2. RDB (Redis DataBase Back File)

#### 4.3.2.1. RDB 原理

![](https://static001.geekbang.org/resource/image/a2/58/a2e5a3571e200cb771ed8a1cd14d5558.jpg)

原理：定期生成所有数据的快照，依赖快照恢复。采用写时复制技术实现，不会阻塞 Redis 读写操作。

- 优点
	- 只有一个 `dump.rdb` 文件，方便持久化。
	- 单个文件方便存储。
	- 性能最大化，fork 子线程备份，主线程不阻塞，IO 最大化.
	- 数据集大时，比 AOF 启动效率高。
- 缺点
	- 间隔时间太长，容易丢失数据。间隔事件太短，对磁盘和 CPU 的抢占率高，容易阻塞主线程。
	- 在写请求高于读请求，且写请求分散在大量数据的情况下，RDB 进行的时候会发生以下风险：
		- 内存资源风险，大量写请求会导致主线程进行内存复制与分离，这是内存使用率会上升。当到达机器容量时，若未配置 swap 策略则会出现 OOM 现象，若配置了会导致内存热点数据与磁盘的交换，导致性能下降。
		- CPU 资源风险：在 CPU 核心少时，主线程已占用了一个线程。fork 的子线程会再占用一个且消耗资源较高。同时还存在其他刷盘，异步关闭文件符这些操作的线程，以及系统内其他的线程。这些线程的争用导致性能下降。

### 4.3.3. AOF 与 RDB 混合模式

Redis4.0 引入了混合模式，支持 RDB 与 AOF 同时使用。新的 AOF 文件前半段是 RDB，后半段是增量的 AOF。采用混合模式既可以利用 RDB 模式备份文件小和全的特性，又可以利用 AOF 一致性高的特性。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220517194638.png)

# 5. 高扩展

## 5.1. Redis Cluster 集群

基于主从，我们可以将节点划分为读写节点以此承担压力，并且利用主从维持了节点之间数据的一致性。之后可以利用哨兵，对主从进行监控，从而可以使节点发生故障时可以切换。但是无论是主从还是哨兵，这里所针对的情况都是数据在一台节点完全存储的，如果单台节点容量存不下了呢？

对于容量负载，我们有两种方式，纵向扩展与横向扩展。纵向扩展即升配机器，横向扩展即增加机器。这二者对比如下：
- 纵向扩展
	- 实施起来简单、直接。
	- 数据量增加，需要的内存也会增加，主线程 fork 子进程时就可能会阻塞。
	- 会受到硬件和成本的限制
	- 数据存在哪儿，客户端访问哪儿，对外访问方式简单。
- 横向扩展
	- 增减机器比升配更容易做到，可以利用多台一般的机器构建一个高可靠的集群。
	- 单台机器容量小，fork 线程压力小。
	- 需要管理多台机器的数据，对于请求分发与数据存储需要做合理划分。
	- 客户端可能需要和多台机器打交道，多机器集群管理问题。

### 5.1.1. Redis Cluster 集群的构建

#### 5.1.1.1. 初始哈希槽的划分

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518160601.png)

在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。映射的规则如下：
1. 首先根据键值对的 key，按照 CRC16 算法计算一个 16 bit 的值。
2. 用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。

将数据划分到哈希槽以后，接下来就是需要将槽分配给 Redis 实例了。有以下几种方式：
- 部署 Redis Cluster 方案时，可以使用 `cluster create` 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。
- 使用 `cluster meet ` 命令手动建立实例间的连接，形成集群，再使用 `cluster addslots` 命令，指定每个实例上的哈希槽个数。按照机器配置或者优先级来手动分配，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。

当哈希槽分配完毕后，每个实例会负责一部分哈希槽。这些集群节点之间还会互相通知自己所负责的哈希槽，这样集群中所有节点都会知道槽的映射关系。

#### 5.1.1.2. 哈希槽的重建

随着数据的的分布不均或者 Redis 实例的增减，出于负载均衡策略就需要将哈希槽需要重新划分。将原属于一个实例的哈希槽分配给其他实例。

数据迁移过程是同步的，迁移一个 key 会同时阻塞源节点和目标节点。

#### 5.1.1.3. 利用哈希槽的原因

1. Redis Cluster 集群采取去中心化的思想，客户端与服务端直连，如果这个访问 key 不在这个节点上，需要服务端有纠错的能力。即节点保存了完整的映射关系，发现错误请求进行 MOVED 响应处理。对于其他集群的实现，如 Codis 的中心化模式，有 proxy 层进行请求分发与节点管理，这就要求 proxy 层也要做到高可用。
2. 增加一层哈希槽，可以把**数据和节点解耦**，key 通过 Hash 计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的 CPU 资源。不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。
3. 当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

Redis 哈希槽的概念和一致性 hash 很像，但它们的侧重点不同：
1. 一致性 hash 通常用来解决当集群节点数变化时，大量缓存无法命中的情况，因为 hashcode/节点数，节点数的变化使得大部分计算结果都发生了变化。一致性 hash 可以保证同一个 key 在节点数变化的时候，大部分“hashcode/节点数”仍然能计算出相同结果。
2. 哈希槽的设计也可以保证这一点：只要把那个节点对应的槽的数据做迁移即可，其余数据不用移动。甚至可以做细粒度的控制，比如机器性能不一样，有的机器可以分配多几个槽。
3. 使用哈希槽避免了从 hash 环上寻找实例节点，中间可能还有从虚拟节点到实际节点的过渡。
4. 使用哈希槽灵活性更强，如将某个实例上的热点槽迁移到其他机器上去，其他槽数据保持不动。

### 5.1.2. Redis Cluster 集群的使用

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220518162735.png)

集群划分好以后就涉及到客户端的使用了，那么客户端如何定位到 Redis 实例呢？客户端通过与 Redis 集群建立连接后，可以知道 Redis 集群中所有哈希槽的映射关系，在操作数据使，按以下步骤进行：
1. 根据操作 key 计算对应的哈希槽。
2. 根据映射信息请求哈希槽所在的实例。对请求到的实例可能出现以下几种情况：
	- 当前实例正常，直接执行命令。
	- 当前 key 所属哈希槽已经不归属于此实例，返回 MOVED 命令响应结果，结果中就包含新实例的访问地址。客户端更新哈希槽映射关系并请求新的实例信息。
	- 当前哈希槽正在迁移，返回 ASK 命令。客户端询问新实例该 key 的情况决定下一步操作。ASK 命令表示两层含义：
		- 第一，表明 Slot 数据还在迁移中。
		- 第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。
		- 和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。这也就是说，ASK 命令的作用只是**让客户端能给新实例发送一次请求**，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。 

# 6. 运行原理

## 6.1. 文件事件模型

- 处理器结构
	- 多个 Socket
	- IO 多路复用程序
	- 文件事件分派器
	- 事件处理器
		- 连接应答处理器
		- 命令请求处理器
		- 命令回复处理器
- 处理器流程
	1. 客户端 Socket 与 Redis 的 Server Socket 请求建立连接
	2. Server Socket 产生 AE_READABLE 事件，事件压入队列中
	3. 文件事件分派器从队列中获取事件，交给连接处理器
	4. 连接处理器建立通信，将 AE_READABLE 事件与命令处理器关联
	5. 客户端发起操作命令，产生 AE_READABLE 事件压入队列。事件分派器将事件交给命令处理器。
	6. 命令处理器处理事件，然后将 socket 的 AE_WRITABLE 事件与回复处理器关联。
	7. 客户端准备好接收结果时，产生 AE_WRITABLE 事件压入队列，命令回复器回复结果。
	8. 最后操作完成，解除命令回复器与 AE_WRITABLE 事件的关联。

## 6.2. Redis 集群模式

- Sential
- Cluster
	- 所有节点互相连接
	- 集群消息通过集群总线通信
	- 节点与节点通过二进制协议通信
	- 客户端与集群节点正常文本协议通信
	- 集群节点不代理查询
	- 数据按 Slot 存储在多个 Redis 实例上。Redis 集群内置 16384 个哈希槽，将 key 按 CRC16 算法计算后对 16484 取余。
	- 集群节点宕机时自动故障转移
	- 可以平滑扩/缩容
- 集群优化策略
	- Master 不做持久化工作
	- 数据交给 Slave 开启持久化备份
	- Master 和 Slave 保持在同一个局域网
	- 避免在压力大大主库上增加从库
	- 主从复制避免图结构，使用单项链表保持节点切换简单

## 6.3. Redis 的内存淘汰策略

- Volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据
- Volatile-ttl：从已设置过期时间的数据集中挑选即将过期的数据
- Volatile-random：从已设置过期时间的数据集中任意挑选数据
- Allkeys-lru：从所有键中挑选最近最少使用的数据
- Allkeys-random：从所有键中中任意挑选数据
- No-enviction：禁止淘汰数据。新写入操作会报错。
- 基于 volatile 策略时若没有键设置了超时时间，那么表现效果和 allkeys 效果一致。

## 6.4. Redis 的过期策略

- 定时删除策略：设定定时器，时间到了就删。此种方式资源消耗大。
- 定期删除策略：每隔一定时间扫描删除。不是扫描所有 key，而是随机抽一部分。
- 惰性删除：操作 key 时如果发现过期了就删除。

## 6.5. Redis 的事务

- Redis事务的特性
	- 事务失败时不支持回滚
	- 一个事务中出现运行错误，其余的命令会继续执行
- Redis 事务的使用
	- MULTI 开启一个事务
	- EXEC 执行一个事务
	- DISCARD 取消一个事务
	- WATCH 提供 Check And Set 的能力，可以监控一个或多个键。一旦某个键被修改/删除，之后的事务就不会执行。

## 6.6. Redis 的 hash 冲突与 hash 扩容

### 6.6.1. Hash 冲突

使用链表保存冲突的数据

### 6.6.2. Hash 扩容

扩容实现：渐进式 hash，从第一个索引开始一点一点的转移数据，具体的实现过程如下：
1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍。
2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中。
	- 拷贝的过程不会是直接全部拷贝，若这么做的话就会阻塞后面的请求。因此 Redis 采取了渐进式 rehash。在拷贝数据时正常处理请求，每处理一个请求就将这个索引上的所有数据拷贝到 hash 表 2 中。等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。
	- 当 rehash 开始后，即使没有请求写入，也会定时当迁移数据到新的 hash 桶中。
3. 释放哈希表 1 的空间。

扩容时机：
Redis 会使用装载因子（load factor）来判断是否需要做 rehash。装载因子的计算方式是：哈希表中所有 entry 的个数除以哈希表的哈希桶个数。Redis 会根据装载因子的两种情况，来触发 rehash 操作：
- 装载因子≥1，同时，哈希表被允许进行 rehash。
	- 如果装载因子等于 1，同时我们假设，所有键值对是平均分布在哈希表的各个桶中的，那么，此时，哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。
	- 如果此时再有新的数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对 RDB 和 AOF 重写造成影响。如果此时，Redis 没有在生成 RDB 和重写 AOF，那么，就可以进行 rehash。否则的话，再有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。
- 装载因子≥5，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。

## 6.7. Redis 分区

- 方案
	- 代理分区：将请求发送给代理，让代理决定对哪个节点读写。如 Twemporxy
	- 客户端分区：由客户端根据一定规则将数据分散到不同节点上
	- 查询路由：客户端随机请求一个 Redis 实例，Redis 转发给正确的 Redis 节点。
- 缺点
	- 涉及多 key 的操作不能很好的支持，如无法直接对分布到不同节点的 key 做交并集计算
	- 同时操作多个 key，无法使用事务
	- 分区的粒度是 key，无法使用一个非常长的排序 key 存储一个数据集
	- 数据处理会变得复杂，如备份时需要收集多个节点的数据
	- 需要处理动态缩扩容的问题

## 6.8. Redis 的缓冲区

### 6.8.1. 客户端输入输出缓冲区

为了避免客户端和服务器端的请求发送和处理速度不匹配，服务器端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区，我们称之为客户端输入缓冲区和输出缓冲区。

输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端，如下图所示：

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519234030.png)

### 6.8.2. 输入输出缓冲区溢出

#### 6.8.2.1. 输入缓冲区溢出

- 写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据。
- 服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。

可以使用 `client list` 命令查看客户端情况，其中有以下几个信息：
- cmd，表示客户端最新执行的命令。
- qbuf，表示输入缓冲区已经使用的大小。
- qbuf-free，表示输入缓冲区尚未使用的大小。

当缓冲区被填满无法接收数据的时候，此时，客户端再写入大量命令的话，就会引起客户端输入缓冲区溢出，Redis 的处理办法就是把客户端连接关闭，结果就是业务程序无法进行数据存取了。

对于客户端的缓冲区，是不支持配置的，为 1GB 的大小。因此我们需要注意客户端写入的速度，防止缓冲区溢出。

#### 6.8.2.2. 输出缓冲区溢出

Redis 为每个客户端设置的输出缓冲区也包括两部分：一部分，是一个大小为 16KB 的固定缓冲空间，用来暂存 OK 响应和出错信息；另一部分，是一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。那什么情况下会发生输出缓冲区溢出呢？ 可能有以下几种：
- 服务器端返回 bigkey 的大量结果。
- 执行了 MONITOR 命令：MONITOR 命令是用来监测 Redis 执行的。执行这个命令之后，就会持续输出监测到的各个命令操作。
- 缓冲区大小设置得不合理：我们可以通过 `client-output-buffer-limit` 配置项，来设置缓冲区的大小。具体设置的内容包括两方面：
	- 设置缓冲区大小的上限阈值。
	- 设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值。
	- 对于不同类型的客户端，也可以有不同的配置。如普通使用的客户端，监听频道订阅的客户端，从库客户端。
		- 普通客户端来说，它每发送完一个请求，会等到请求结果返回后，再发送下一个请求，这种发送方式称为阻塞式发送。在这种情况下，如果不是读取体量特别大的 bigkey，服务器端的输出缓冲区一般不会被阻塞的。所以，我们通常把普通客户端的缓冲区大小限制，以及持续写入量限制、持续写入时间限制都设置为 0，也就是不做限制。
		- 对于订阅客户端来说，一旦订阅的 Redis 频道有消息了，服务器端都会通过输出缓冲区把消息发给客户端。所以订阅客户端和服务器间的消息发送方式，不属于阻塞式发送。不过，如果频道消息较多的话，也会占用较多的输出缓冲区空间。因此，我们会给订阅客户端设置缓冲区大小限制、缓冲区持续写入量限制，以及持续写入时间限制。

### 6.8.3. 主从集群缓冲区

### 6.8.4. 全量复制缓冲区

主从集群间的数据复制包括全量复制和增量复制两种。全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。无论在哪种形式的复制中，为了保证主从节点的数据一致，都会用到缓冲区。

在全量复制过程中，主节点在向从节点传输 RDB 文件的同时，会继续接收客户端发送的写命令请求。这些写命令就会先保存在复制缓冲区中，等 RDB 文件传输完成后，再发送给从节点去执行。主节点上会为每个从节点都维护一个复制缓冲区，来保证主从节点间的数据同步。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519235134.png)

针对主从复制的缓冲区，我们可以做以下几点考虑：
- 控制主节点保存的数据量大小。按通常的使用经验，我们会把主节点的数据量控制在 2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区累积过多命令。
- 使用 ` client-output-buffer-limit` 配置项，来设置合理的复制缓冲区大小。设置的依据，就是主节点的数据量大小、主节点的写负载压力和主节点本身的内存大小。如估算每秒的请求写入量，每条请求的数据量来预设缓冲区大小。
- 主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和。如果集群中的从节点数非常多的话，主节点的内存开销就会非常大。所以，我们还必须得控制和主节点连接的从节点个数，不要使用大规模的主从集群。

### 6.8.5. 增量复制缓冲区

主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步。

这个增量缓冲区就是我们提到的`repl_backlog_buffer`。复制积压缓冲区是一个大小有限的环形缓冲区。当主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据。如果从节点还没有同步这些旧命令数据，就会造成主从节点间重新开始执行全量复制。

![](https://varg-my-images.oss-cn-beijing.aliyuncs.com/img/20220519235655.png)

从本质上看，缓冲区溢出，无非就是三个原因：命令数据发送过快过大；命令数据处理较慢；缓冲区空间过小。明白了这个，我们就可以有针对性地拿出应对策略了。
- 针对命令数据发送过快过大的问题，对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。
- 针对命令数据处理较慢的问题，解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作。
- 针对缓冲区空间过小的问题，解决方案就是使用 client-output-buffer-limit 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，我们不要忘了，输入缓冲区的大小默认是固定的，我们无法通过配置来修改它，除非直接去修改 Redis 源码。